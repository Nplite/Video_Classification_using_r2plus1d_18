{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch torchvision moviepy opencv-python-headless streamlit scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Prepare Your Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize your dataset:\n",
    "\n",
    "Create directories where each subdirectory corresponds to an action class (e.g., running, walking, etc.).\n",
    "Inside each directory, add corresponding video files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example\n",
    "|\n",
    "dataset/\n",
    "├── running/\n",
    "│   ├── video1.mp4\n",
    "│   ├── video2.mp4\n",
    "├── walking/\n",
    "    ├── video1.mp4\n",
    "    ├── video2.mp4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Split Dataset into Training and Validation:\n",
    "\n",
    "Use a ratio of 80% for training and 20% for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Write Dataset Class to Load Videos\n",
    "\n",
    "Create a custom VideoDataset class that will load video frames, apply transformations, and return the data as tensors for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, label_map, frames_per_clip=32, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.frames_per_clip = frames_per_clip\n",
    "        self.label_map = label_map\n",
    "        self.videos = self._get_video_paths()\n",
    "\n",
    "    def _get_video_paths(self):\n",
    "        videos = []\n",
    "        for class_name in os.listdir(self.root_dir):\n",
    "            class_dir = os.path.join(self.root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for video in os.listdir(class_dir):\n",
    "                    videos.append((os.path.join(class_dir, video), self.label_map[class_name]))\n",
    "        return videos\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.videos[idx]\n",
    "        frames = self._extract_frames(video_path)\n",
    "        if self.transform:\n",
    "            frames = [self.transform(frame) for frame in frames]\n",
    "        frames = torch.stack(frames).permute(1, 0, 2, 3)  # (C, T, H, W)\n",
    "        return frames, label\n",
    "\n",
    "    def _extract_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while len(frames) < self.frames_per_clip:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (112, 112))  # Resize frames to match model input\n",
    "            frames.append(torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1) / 255.0)  # Normalize\n",
    "        cap.release()\n",
    "        return frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Define Data Loaders\n",
    "Split the data into training and validation sets and create data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_loaders(root_dir, batch_size=4):\n",
    "    label_map = {class_name: i for i, class_name in enumerate(os.listdir(root_dir))}\n",
    "    dataset = VideoDataset(root_dir, label_map, transform=transforms.Normalize((0.5,), (0.5,)))\n",
    "\n",
    "    train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, label_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Fine-tune the Pre-trained r2plus1d_18 Model\n",
    "Modify the final layer to match the number of classes in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "def create_model(num_classes):\n",
    "    model = models.video.r2plus1d_18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)  # Update the last layer\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Define Training Loop\n",
    "Here’s the training loop where the model learns from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss/len(train_loader)}\")\n",
    "\n",
    "        validate(model, val_loader, device)\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Train the Model\n",
    "Initialize the model, load the data, and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alluvium/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/alluvium/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R2Plus1D_18_Weights.KINETICS400_V1`. You can also use `weights=R2Plus1D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/r2plus1d_18-91a641e6.pth\" to /home/alluvium/.cache/torch/hub/checkpoints/r2plus1d_18-91a641e6.pth\n",
      "100%|██████████| 120M/120M [00:04<00:00, 30.3MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.4484711244895861\n",
      "Validation Loss: 0.11833928966079839, Accuracy: 96.15384615384616%\n",
      "Epoch 2/10, Training Loss: 0.21936393480796318\n",
      "Validation Loss: 0.06307444193516858, Accuracy: 98.71794871794872%\n",
      "Epoch 3/10, Training Loss: 0.23227855880055334\n",
      "Validation Loss: 0.07171267224475741, Accuracy: 98.71794871794872%\n",
      "Epoch 4/10, Training Loss: 0.23210832130696094\n",
      "Validation Loss: 0.09051896380260586, Accuracy: 98.71794871794872%\n",
      "Epoch 5/10, Training Loss: 0.11675865398478005\n",
      "Validation Loss: 0.070965994335711, Accuracy: 98.71794871794872%\n",
      "Epoch 6/10, Training Loss: 0.12450954276923236\n",
      "Validation Loss: 0.1131224851065781, Accuracy: 96.15384615384616%\n",
      "Epoch 7/10, Training Loss: 0.06309299194253981\n",
      "Validation Loss: 0.07285855153459124, Accuracy: 98.71794871794872%\n",
      "Epoch 8/10, Training Loss: 0.021976920212135202\n",
      "Validation Loss: 0.062271978156059046, Accuracy: 98.71794871794872%\n",
      "Epoch 9/10, Training Loss: 0.13053429370002168\n",
      "Validation Loss: 0.06104815102880821, Accuracy: 98.71794871794872%\n",
      "Epoch 10/10, Training Loss: 0.07422867543275723\n",
      "Validation Loss: 0.04327523097163066, Accuracy: 98.71794871794872%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader, val_loader, label_map = get_loaders('/home/alluvium/Desktop/Video_classification/dataset')\n",
    "model = create_model(num_classes=len(label_map))\n",
    "\n",
    "train_model(model, train_loader, val_loader, device, epochs=10)\n",
    "torch.save(model.state_dict(), 'action_recognition_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Test the Model on a Video File\n",
    "Use the trained model to make predictions on a new video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Action: ThrowDiscus, Confidence: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76067/3481132540.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('action_recognition_model.pth', map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def load_video_frames(video_path, frames_per_clip=32):\n",
    "    \"\"\"Load frames from the video and prepare them for the model.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while len(frames) < frames_per_clip:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize the frame to match the model's input size (112x112)\n",
    "        frame = cv2.resize(frame, (112, 112))\n",
    "        \n",
    "        # Convert frame to tensor and normalize (values between 0 and 1)\n",
    "        frame = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1) / 255.0  # (C, H, W)\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # If not enough frames, pad with the last frame\n",
    "    while len(frames) < frames_per_clip:\n",
    "        frames.append(frames[-1].clone())\n",
    "\n",
    "    # Stack frames along the time dimension and add batch dimension\n",
    "    video_tensor = torch.stack(frames, dim=1)  # (C, T, H, W)\n",
    "    return video_tensor.unsqueeze(0)  # (1, C, T, H, W)\n",
    "\n",
    "\n",
    "def predict_on_video(model, video_path, device, label_map, threshold=0.5):\n",
    "    \"\"\"Predict the action in a single video.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load the video frames\n",
    "    inputs = load_video_frames(video_path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "        class_name = list(label_map.keys())[list(label_map.values()).index(predicted.item())]\n",
    "\n",
    "        if confidence.item() < threshold:\n",
    "            print(f\"Predicted Action: Unknown, Confidence: {confidence.item():.2f}\")\n",
    "        else:\n",
    "            print(f\"Predicted Action: {class_name}, Confidence: {confidence.item():.2f}\")\n",
    "\n",
    "# Load the model and make predictions\n",
    "model.load_state_dict(torch.load('action_recognition_model.pth', map_location=device))\n",
    "vid_path = 'dataset/JavelineThrow/v_JavelinThrow_g02_c04.avi'\n",
    "predict_on_video(model, vid_path, device, label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
